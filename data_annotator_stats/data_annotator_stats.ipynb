{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inter-annotator agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann11 = \"intern_responses/medqa_reasoning/1.3-Table 1.csv\"\n",
    "ann12 = \"intern_responses/medqa_reasoning/1.2-Table 1.csv\"\n",
    "ann21 = \"intern_responses/medqa_reasoning/2.3-Table 1.csv\"\n",
    "ann22 = \"intern_responses/medqa_reasoning/2.2-Table 1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1 = \"intern_responses/medqa_answers/1.2-Table 1.csv\"\n",
    "ann2 = \"intern_responses/medqa_answers/1.3-Table 1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = pd.read_csv(ann11)\n",
    "df12 = pd.read_csv(ann12)\n",
    "df21 = pd.read_csv(ann21)\n",
    "df22 = pd.read_csv(ann22)\n",
    "\n",
    "df1 = pd.concat([df11,df21]).reset_index()\n",
    "df2 = pd.concat([df12,df22]).reset_index()\n",
    "\n",
    "df1 = df1.drop(['Comparison','index'], axis = 1)\n",
    "df2 = df2.drop(['Comparison','index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(ann1)\n",
    "df2 = pd.read_csv(ann2)\n",
    "\n",
    "df1 = df1.drop('Comparison', axis = 1)\n",
    "df2 = df2.drop('Comparison', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(ann1)\n",
    "df2 = pd.read_csv(ann2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "100 100\n"
     ]
    }
   ],
   "source": [
    "print(df1.isnull().any().any())\n",
    "print(df2.isnull().any().any())\n",
    "print(len(df1),len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_null_df1 = df1[df1.isnull().any(axis=1)].index\n",
    "indices_null_df2 = df2[df2.isnull().any(axis=1)].index\n",
    "\n",
    "# Find the union of indices with null values in either dataframe\n",
    "indices_to_drop = set(indices_null_df1) | set(indices_null_df2)\n",
    "\n",
    "# Drop rows with null values from both dataframes\n",
    "df1 = df1.drop(indices_to_drop)\n",
    "df2 = df2.drop(indices_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "94 94\n"
     ]
    }
   ],
   "source": [
    "print(df1.isnull().any().any())\n",
    "print(df2.isnull().any().any())\n",
    "print(len(df1),len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 5-point\n",
    "\n",
    "likert_num_dict = {'strongly agree' : 4, 'agree' : 3, 'neutral' : 2, 'disagree' : 1 , 'strongly disagree' : 0}\n",
    "\n",
    "df1['codex_score'] = df1['Correctness of Option 1'].apply(lambda x : likert_num_dict[x.lower()])\n",
    "df1['kj_score'] = df1['Correctness of Option 2'].apply(lambda x : likert_num_dict[x.lower()])\n",
    "\n",
    "df2['codex_score'] = df2['Correctness of Option 1'].apply(lambda x : likert_num_dict[x.lower()])\n",
    "df2['kj_score'] = df2['Correctness of Option 2'].apply(lambda x : likert_num_dict[x.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 2-point\n",
    "\n",
    "bin_num_dict = {'correct' : 1, 'incorrrect' : 0}\n",
    "\n",
    "df1['codex_score'] = df1['Correctness of Option 1'].apply(lambda x : bin_num_dict[x.lower()])\n",
    "df1['kj_score'] = df1['Correctness of Option 2'].apply(lambda x : bin_num_dict[x.lower()])\n",
    "\n",
    "df2['codex_score'] = df2['Correctness of Option 1'].apply(lambda x : bin_num_dict[x.lower()])\n",
    "df2['kj_score'] = df2['Correctness of Option 2'].apply(lambda x : bin_num_dict[x.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1_scores = np.array(pd.concat([df1['codex_score'], df1['kj_score']]))\n",
    "ann2_scores = np.array(pd.concat([df2['codex_score'], df2['kj_score']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1879049676025918\n"
     ]
    }
   ],
   "source": [
    "res = cohen_kappa_score(ann1_scores, ann2_scores)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18888066332310346"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = kendalltau(ann1_scores, ann2_scores)\n",
    "res.statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(ann1_scores, ann2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 24,  36],\n",
       "       [ 28, 100]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1_pd = pd.concat([df1['codex_score'], df1['kj_score']])\n",
    "ann2_pd = pd.concat([df2['codex_score'], df2['kj_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    128\n",
       "0     60\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ann1_pd.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    136\n",
       "0     52\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ann2_pd.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codex_nonmatching_questions = list(df1.loc[df1['codex_score'] != df2['codex_score'],\"Questions\"])\n",
    "options = list(df1.loc[df1['codex_score'] != df2['codex_score'],\"Option 1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A 32-year-old woman comes to the physician because of a 3-week history of intermittent loose stools and a 1.2-kg (2.6-lb) weight loss. She immigrated to the US from Uganda 6 weeks ago. Abdominal examination shows diffuse tenderness with no guarding or rebound. The liver is firm and palpable 3 cm below the right costal margin, and the spleen is palpable just below the left costal margin. Her leukocyte count is 12,800/mm3 (12% eosinophils). Stool culture shows several oval-shaped eggs with lateral spines. Microscopic examination of a liver biopsy specimen shows granulomatous inflammation with periportal fibrosis. Exposure to what substance is most likely to have played a role in the development of this patient's symptoms?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "codex_nonmatching_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Schistosomiasis.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1 = \"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/intern_responses/medqa_reasoning/1.1-Table 1.csv\"\n",
    "ann2 = \"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/intern_responses/medqa_reasoning/1.2-Table 1.csv\"\n",
    "ann3 = \"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/intern_responses/medqa_reasoning/1.3-Table 1.csv\"\n",
    "ann4 = \"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/intern_responses/medqa_reasoning/1.4-Table 1.csv\"\n",
    "\n",
    "anns = [ann1,ann2,ann3,ann4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1 = \"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/intern_responses/medqa_answers/1.2-Table 1.csv\"\n",
    "ann2 = \"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/intern_responses/medqa_answers/1.3-Table 1.csv\"\n",
    "\n",
    "anns = [ann1,ann2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1 = \"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/intern_responses/real_answers/1.1-Table 1.csv\"\n",
    "ann2 = \"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/intern_responses/real_answers/1.2-Table 1.csv\"\n",
    "ann3 = \"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/intern_responses/real_answers/1.3-Table 1.csv\"\n",
    "\n",
    "anns = [ann1,ann2,ann3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1 = \"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/intern_responses/real_reasoning/1.3-Table 1.csv\"\n",
    "\n",
    "anns = [ann1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for i in range(len(anns)):\n",
    "    dfs.append(pd.read_csv(anns[i]))\n",
    "\n",
    "    cols = dfs[-1].columns\n",
    "    if 'Comparison' in cols:\n",
    "        dfs[-1] = dfs[-1].drop('Comparison', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_null_dfs = []\n",
    "indices_to_drop = set()\n",
    "for i in range(len(dfs)):\n",
    "    indices_null_df = dfs[i][dfs[i].isnull().any(axis=1)].index\n",
    "\n",
    "    indices_to_drop = indices_to_drop | set(indices_null_df)\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    dfs[i] = dfs[i].drop(indices_to_drop).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-point\n",
    "score_num_dict = {'strongly agree' : 4, 'agree' : 3, 'neutral' : 2, 'disagree' : 1 , 'strongly disagree' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-point\n",
    "score_num_dict = {'correct' : 1, 'incorrrect' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    dfs[i]['codex_score'] = dfs[i]['Correctness of Option 1'].apply(lambda x : score_num_dict[x.lower()])\n",
    "    dfs[i]['kj_score'] = dfs[i]['Correctness of Option 2'].apply(lambda x : score_num_dict[x.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "codex_scores = []\n",
    "kj_scores = []\n",
    "\n",
    "for i in range(len(dfs[0])):\n",
    "    for j in range(len(dfs)):\n",
    "        codex_scores.append(dfs[j].loc[i,'codex_score'])\n",
    "        kj_scores.append(dfs[j].loc[i,'kj_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss_scores = {'codex_scores' : codex_scores, 'kj_scores' : kj_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import statistics, random\n",
    "\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "\n",
    "from ast import literal_eval\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_annotations = []\n",
    "annotation_counts = {}\n",
    "scores = {}\n",
    "raw_scores = {}\n",
    "\n",
    "def get_files_in_folder(dir_path):\n",
    "    res = []\n",
    "    for (_, _, file_names) in walk(dir_path):\n",
    "        res.extend(file_names)\n",
    "    return res\n",
    "\n",
    "def read_annotation_export(file, normalized_names):\n",
    "\n",
    "    converters={\"labels\": literal_eval, \"input_background\": literal_eval}\n",
    "    data = pd.read_csv(file, converters=converters)\n",
    "    column_names = list(data.columns)\n",
    "    \n",
    "    domain_scores = {}\n",
    "    count = 0\n",
    "    for index, row in data.iterrows():\n",
    "        annotation = {}\n",
    "        annotation[\"input\"] = row[\"input\"]\n",
    "        annotation[\"predictions\"] = {}\n",
    "        annotation[\"doc_id\"] = row[\"doc_id\"]\n",
    "        annotation[\"relevant_document\"] = True\n",
    "        if row[\"passage_assessment\"] == \"No relevant link\":\n",
    "            annotation[\"relevant_document\"] = False\n",
    "\n",
    "        for column_name in column_names:\n",
    "            if column_name.startswith(\"model_scoring_\"):\n",
    "                model_name = column_name.replace(\"model_scoring_\", \"\")\n",
    "                normalized_model_name = normalized_names[model_name]\n",
    "                #normalized_model_name = model_name\n",
    "                annotation[\"predictions\"][normalized_model_name] = {\n",
    "                                                        \"prediction\": row[model_name+\"_prediction\"],\n",
    "                                                        \"score\": row[column_name]\n",
    "                                                    }\n",
    "                if normalized_model_name not in annotation_counts:\n",
    "                    annotation_counts[normalized_model_name] = 0\n",
    "                    scores[normalized_model_name] = 0\n",
    "                if normalized_model_name not in domain_scores:\n",
    "                    domain_scores[normalized_model_name] = 0\n",
    "                if normalized_model_name not in raw_scores:\n",
    "                    raw_scores[normalized_model_name] = []\n",
    "\n",
    "                annotation_counts[normalized_model_name] += 1\n",
    "                score = float(row[column_name].replace(\"%\", \"\"))\n",
    "                scores[normalized_model_name] += score\n",
    "                domain_scores[normalized_model_name]  += score\n",
    "                raw_scores[normalized_model_name].append(score)\n",
    "        count+=1\n",
    "\n",
    "        all_annotations.append(copy.deepcopy(annotation))\n",
    "\n",
    "    for k,v in domain_scores.items():\n",
    "        domain_scores[k] = v/count\n",
    "\n",
    "    return domain_scores, count\n",
    "\n",
    "def plot(data, name):\n",
    "\n",
    "    df = pd.DataFrame(list(data.items()), columns=['Category', 'Value'])\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))  # Set the figure size (optional)\n",
    "    ax = sns.barplot(x=\"Category\", y=\"Value\", data=df)\n",
    "\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Average Score')\n",
    "    plt.title(name.upper())\n",
    "\n",
    "    # Annotate bars with values on top (with 2 decimal points)\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', fontsize=12, color='black', xytext=(0, 10),\n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout for better spacing\n",
    "    plt.savefig(name+'.png')\n",
    "\n",
    "def make_multiple_plots(datasets, titles, name):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Create a single figure with a 2x3 grid for the charts\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for i, data in enumerate(datasets, start=1):\n",
    "        df = pd.DataFrame(list(data.items()), columns=['Category', 'Value'])\n",
    "\n",
    "        plt.subplot(2, 2, i)  # 2 rows, 3 columns, index i\n",
    "        ax = sns.barplot(x=\"Category\", y=\"Value\", data=df)\n",
    "\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('Average Score')\n",
    "        plt.title(titles[i - 1])\n",
    "\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                        ha='center', va='center', fontsize=8, color='black', xytext=(0, 10),\n",
    "                        textcoords='offset points')\n",
    "\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(name+'.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# So 100K is almost as good as 20m.  W. Morgan’s slides ( https://cs.stanford.edu/people/wmorgan/sigtest.pdf ) say that 1K is \"typical\".\n",
    "# Philipp Koehn ( https://aclanthology.org/W04-3250/ ) also uses 1K.\n",
    "# Here we use 100K which is kind of slow. If you have bigger data and/or are in a hurry, probably 1K is fine.\n",
    "FISHER_RANDOMIZATION_TRIALS = 100000\n",
    "\n",
    "# Fisher's Randomization Test as described in \"A Comparison of Statistical Significance Tests for Information Retrieval Evaluation\" by Mark D. Smucker, James Allan, and Ben Carterette. CIKM 2007.\n",
    "# The same method is described in W. Morgan’s slides ( https://cs.stanford.edu/people/wmorgan/sigtest.pdf ) under the name \"randomization test\".\n",
    "#  W. Morgan adds “+1” that Morgan adds to the numerator and denominator (the bullet starting with “Actually” on slide 10 and ending with “not that it matters for, say, R ≥ 19”).\n",
    "#  We do the same here because Morgan’s case for including it (to be \"statistically valid\") seems solid -- but as noted, it doesn't matter much.\n",
    "# Philipp Koehn ( https://aclanthology.org/W04-3250/ ) calls the method \"paired bootstrap resampling\".\n",
    "# This implementation assumes the metric is computed as the mean of a list of scores, one per sample.  Fisher's Randomization Test can also be run on other metrics, but then you\n",
    "#  need to apply the full metric computation to the x and y vectors below.\n",
    "def fisher_randomization(list_of_scores_a, list_of_scores_b):\n",
    "    mean_a = statistics.mean(list_of_scores_a)\n",
    "    mean_b = statistics.mean(list_of_scores_b)\n",
    "    actual_abs_difference = abs(mean_a-mean_b)\n",
    " \n",
    "    num_scores = len(list_of_scores_a)\n",
    "    count_trial_differences_greater_or_equal_to_actual = 0\n",
    "    for _ in range(FISHER_RANDOMIZATION_TRIALS):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for i in range(num_scores):\n",
    "            score_a = list_of_scores_a[i]\n",
    "            score_b = list_of_scores_b[i]\n",
    "            flip = random.choice([True, False])\n",
    "            if flip:\n",
    "                score_x = score_b\n",
    "                score_y = score_a\n",
    "            else:\n",
    "                score_x = score_a\n",
    "                score_y = score_b\n",
    "            sum_x += score_x\n",
    "            sum_y += score_y\n",
    "        mean_x = sum_x / num_scores\n",
    "        mean_y = sum_y / num_scores\n",
    "        trial_abs_difference = abs(mean_x-mean_y)\n",
    "        if trial_abs_difference >= actual_abs_difference:\n",
    "            count_trial_differences_greater_or_equal_to_actual += 1\n",
    "    p = (count_trial_differences_greater_or_equal_to_actual + 1) / (FISHER_RANDOMIZATION_TRIALS + 1)\n",
    "    return p, mean_a, mean_b\n",
    "\n",
    "\n",
    "def test_all_pairs(raw_scores):\n",
    "    labels = sorted(raw_scores.keys())\n",
    "\n",
    "    for system_a in labels:\n",
    "        for system_b in labels:\n",
    "            if system_a < system_b:\n",
    "                p, mean_a, mean_b = fisher_randomization(raw_scores[system_a], raw_scores[system_b])\n",
    "                print(f\"{system_a} ({mean_a:.4f}) vs. {system_b} ({mean_b:.4f})\")\n",
    "                if (p <= 0.05):\n",
    "                    print(f\"p = {p:.4f}: significant\")\n",
    "                else:\n",
    "                    print(f\"p = {p:.4f}: not significant\")\n",
    "                print()\n",
    "\n",
    "\n",
    "def load_normalized_names(file):\n",
    "    normalized_names = {}\n",
    "    with open(file) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in csvreader:\n",
    "            normalized_names[row[0].strip()] = row[1].strip()\n",
    "    \n",
    "    return normalized_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_scores = {'codex_scores' : codex_scores[10:20], 'kj_scores' : kj_scores[10:20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0364583333333335"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(codex_scores[''])/len(codex_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1458333333333335"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(kj_scores)/len(kj_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codex_scores (3.0000) vs. kj_scores (3.0000)\n",
      "p = 1.0000: not significant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_all_pairs(ss_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [5, 5, 5, 4, 4, 4], 'b': [1, 1, 1, 2, 2, 2]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_scores = {'a' : [5,5,5,4,4,4,],'b' : [1,1,1,2,2,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codex_scores (3.0000) vs. kj_scores (3.0000)\n",
      "p = 1.0000: not significant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datasets = []\n",
    "# titles = []\n",
    "# raw_scores_for_datasets = []\n",
    "\n",
    "# annotation_name = \"aug22\"\n",
    "# normalized_names = load_normalized_names(\"normalized_names-\"+annotation_name+\".tsv\")\n",
    "\n",
    "# DATA_FOLDER = \"./\"+annotation_name+\"/\"\n",
    "# files = get_files_in_folder(DATA_FOLDER)\n",
    "# for file in files:\n",
    "#     if file.endswith(\".csv\"):# and \"SFSF\" not in file:\n",
    "#         domain_scores, count = read_annotation_export(DATA_FOLDER+file, normalized_names)\n",
    "#         domain = \"SD\"\n",
    "#         if \"SFSF\" in file:\n",
    "#             domain = \"SFSF\"\n",
    "#         if \"MM\" in file:\n",
    "#             domain = \"MM\"\n",
    "#         if \"Finance\" in file:\n",
    "#             domain = \"Finance\"\n",
    "#         datasets.append(copy.deepcopy(domain_scores))\n",
    "#         titles.append(annotation_name+\"-\" + domain +\"-with-\"+ str(count)+\"-annotations\")\n",
    "\n",
    "# no_of_annotations = 0\n",
    "# for k,v in annotation_counts.items():\n",
    "#     scores[k] = scores[k]/v\n",
    "#     print(k,v)\n",
    "#     no_of_annotations = v\n",
    "\n",
    "# #make_multiple_plots(datasets, titles, \"domain-wise-annotations-\" + annotation_name)\n",
    "\n",
    "# #plot(scores,annotation_name+\"-with-\"+ str(no_of_annotations)+\"-annotations\")\n",
    "\n",
    "# print(datasets)\n",
    "# print(titles)\n",
    "\n",
    "# test_all_pairs(ss_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mismatched data points in IAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1 = \"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/intern_responses/medqa_answers/1.2-Table 1.csv\"\n",
    "ann2 = \"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/intern_responses/medqa_answers/1.3-Table 1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(ann1)\n\u001b[1;32m      2\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(ann2)\n\u001b[1;32m      3\u001b[0m df1 \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComparison\u001b[39m\u001b[38;5;124m'\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(ann1)\n",
    "df2 = pd.read_csv(ann2)\n",
    "df1 = df1.drop('Comparison', axis = 1)\n",
    "df2 = df2.drop('Comparison', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(ann1)\n",
    "df2 = pd.read_csv(ann2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(df1.isnull().any().any())\n",
    "print(df2.isnull().any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_null_df1 = df1[df1.isnull().any(axis=1)].index\n",
    "indices_null_df2 = df2[df2.isnull().any(axis=1)].index\n",
    "\n",
    "# Find the union of indices with null values in either dataframe\n",
    "indices_to_drop = set(indices_null_df1) | set(indices_null_df2)\n",
    "\n",
    "# Drop rows with null values from both dataframes\n",
    "df1 = df1.drop(indices_to_drop)\n",
    "df2 = df2.drop(indices_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 5-point\n",
    "\n",
    "likert_num_dict = {'strongly agree' : 4, 'agree' : 3, 'neutral' : 2, 'disagree' : 1 , 'strongly disagree' : 0}\n",
    "\n",
    "df1['codex_score'] = df1['Correctness of Option 1'].apply(lambda x : likert_num_dict[x.lower()])\n",
    "df1['kj_score'] = df1['Correctness of Option 2'].apply(lambda x : likert_num_dict[x.lower()])\n",
    "\n",
    "df2['codex_score'] = df2['Correctness of Option 1'].apply(lambda x : likert_num_dict[x.lower()])\n",
    "df2['kj_score'] = df2['Correctness of Option 2'].apply(lambda x : likert_num_dict[x.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 2-point\n",
    "\n",
    "bin_num_dict = {'correct' : 1, 'incorrrect' : 0}\n",
    "\n",
    "df1['codex_score'] = df1['Correctness of Option 1'].apply(lambda x : bin_num_dict[x.lower()])\n",
    "df1['kj_score'] = df1['Correctness of Option 2'].apply(lambda x : bin_num_dict[x.lower()])\n",
    "\n",
    "df2['codex_score'] = df2['Correctness of Option 1'].apply(lambda x : bin_num_dict[x.lower()])\n",
    "df2['kj_score'] = df2['Correctness of Option 2'].apply(lambda x : bin_num_dict[x.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Option 1</th>\n",
       "      <th>Approach 1</th>\n",
       "      <th>Correctness of Option 1</th>\n",
       "      <th>Option 2</th>\n",
       "      <th>Approach 2</th>\n",
       "      <th>Correctness of Option 2</th>\n",
       "      <th>codex_score</th>\n",
       "      <th>kj_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 32-year-old woman comes to the physician bec...</td>\n",
       "      <td>Schistosomiasis.</td>\n",
       "      <td>Codex</td>\n",
       "      <td>Correct</td>\n",
       "      <td>contaminated water in Uganda.</td>\n",
       "      <td>KJ</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 30-year-old woman comes to the physician bec...</td>\n",
       "      <td>endometriosis</td>\n",
       "      <td>Codex</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Initiate hormonal treatment, consider laparosc...</td>\n",
       "      <td>KJ</td>\n",
       "      <td>Incorrrect</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A 43-year-old man with a history of schizophre...</td>\n",
       "      <td>Risperidone.</td>\n",
       "      <td>Codex</td>\n",
       "      <td>Correct</td>\n",
       "      <td>haloperidol.</td>\n",
       "      <td>KJ</td>\n",
       "      <td>Incorrrect</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A healthy 23-year-old male is undergoing an ex...</td>\n",
       "      <td>Muscles.</td>\n",
       "      <td>Codex</td>\n",
       "      <td>Incorrrect</td>\n",
       "      <td>Muscles that are active during exercise.</td>\n",
       "      <td>KJ</td>\n",
       "      <td>Incorrrect</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A one-day-old male is evaluated in the hospita...</td>\n",
       "      <td>Congenital bowel obstruction (pyloric stenosis).</td>\n",
       "      <td>Codex</td>\n",
       "      <td>Incorrrect</td>\n",
       "      <td>intestinal atresia.</td>\n",
       "      <td>KJ</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>A 67-year-old man with transitional cell carci...</td>\n",
       "      <td>Ototoxicity.</td>\n",
       "      <td>Codex</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Ototoxicity due to Cisplatin.</td>\n",
       "      <td>KJ</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A 20-year-old man comes to the physician becau...</td>\n",
       "      <td>Neurofibromatosis type 2 (NF2).</td>\n",
       "      <td>Codex</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Neurofibromatosis type 2.</td>\n",
       "      <td>KJ</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A junior orthopaedic surgery resident is compl...</td>\n",
       "      <td>The resident should document the complication ...</td>\n",
       "      <td>Codex</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The resident should explain to the attending p...</td>\n",
       "      <td>KJ</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>A medical research study is evaluating an inve...</td>\n",
       "      <td>20</td>\n",
       "      <td>Codex</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1.79.</td>\n",
       "      <td>KJ</td>\n",
       "      <td>Incorrrect</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>A 3-week-old male newborn is brought to the ph...</td>\n",
       "      <td>Immobilize the foot in a neutral position usin...</td>\n",
       "      <td>Codex</td>\n",
       "      <td>Incorrrect</td>\n",
       "      <td>serial manipulations.</td>\n",
       "      <td>KJ</td>\n",
       "      <td>Incorrrect</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Questions  \\\n",
       "1   A 32-year-old woman comes to the physician bec...   \n",
       "2   A 30-year-old woman comes to the physician bec...   \n",
       "3   A 43-year-old man with a history of schizophre...   \n",
       "4   A healthy 23-year-old male is undergoing an ex...   \n",
       "5   A one-day-old male is evaluated in the hospita...   \n",
       "..                                                ...   \n",
       "95  A 67-year-old man with transitional cell carci...   \n",
       "96  A 20-year-old man comes to the physician becau...   \n",
       "97  A junior orthopaedic surgery resident is compl...   \n",
       "98  A medical research study is evaluating an inve...   \n",
       "99  A 3-week-old male newborn is brought to the ph...   \n",
       "\n",
       "                                             Option 1 Approach 1  \\\n",
       "1                                    Schistosomiasis.      Codex   \n",
       "2                                       endometriosis      Codex   \n",
       "3                                        Risperidone.      Codex   \n",
       "4                                            Muscles.      Codex   \n",
       "5    Congenital bowel obstruction (pyloric stenosis).      Codex   \n",
       "..                                                ...        ...   \n",
       "95                                       Ototoxicity.      Codex   \n",
       "96                    Neurofibromatosis type 2 (NF2).      Codex   \n",
       "97  The resident should document the complication ...      Codex   \n",
       "98                                                 20      Codex   \n",
       "99  Immobilize the foot in a neutral position usin...      Codex   \n",
       "\n",
       "   Correctness of Option 1                                           Option 2  \\\n",
       "1                  Correct                      contaminated water in Uganda.   \n",
       "2                  Correct  Initiate hormonal treatment, consider laparosc...   \n",
       "3                  Correct                                       haloperidol.   \n",
       "4               Incorrrect           Muscles that are active during exercise.   \n",
       "5               Incorrrect                                intestinal atresia.   \n",
       "..                     ...                                                ...   \n",
       "95                 Correct                      Ototoxicity due to Cisplatin.   \n",
       "96                 Correct                          Neurofibromatosis type 2.   \n",
       "97                 Correct  The resident should explain to the attending p...   \n",
       "98                 Correct                                              1.79.   \n",
       "99              Incorrrect                              serial manipulations.   \n",
       "\n",
       "   Approach 2 Correctness of Option 2  codex_score  kj_score  \n",
       "1          KJ                 Correct            1         1  \n",
       "2          KJ              Incorrrect            1         0  \n",
       "3          KJ              Incorrrect            1         0  \n",
       "4          KJ              Incorrrect            0         0  \n",
       "5          KJ                 Correct            0         1  \n",
       "..        ...                     ...          ...       ...  \n",
       "95         KJ                 Correct            1         1  \n",
       "96         KJ                 Correct            1         1  \n",
       "97         KJ                 Correct            1         1  \n",
       "98         KJ              Incorrrect            1         0  \n",
       "99         KJ              Incorrrect            0         0  \n",
       "\n",
       "[94 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "codex_mismatch = df1.loc[df2['codex_score'] != df1['codex_score'],['Questions','Option 1','codex_score']]\n",
    "codex_mismatch[\"codex_score_2\"] = df2.loc[codex_mismatch.index,'codex_score']\n",
    "# codex_mismatch = codex_mismatch.reset_index()\n",
    "\n",
    "kj_mismatch = df1.loc[df2['kj_score'] != df1['kj_score'],['Questions','Option 1','kj_score']]\n",
    "kj_mismatch[\"kj_score_2\"] = df2.loc[kj_mismatch.index,'kj_score']\n",
    "# kj_mismatch = kj_mismatch.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "codex_mismatch.to_csv(\"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/codex_mismatch_medqa_answers.csv\", index=False)\n",
    "kj_mismatch.to_csv(\"/dccstor/ojasgr/scripts/approaches/data_annotator_stats/kj_mismatch_medqa_answers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
